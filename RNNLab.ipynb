{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d607a9c-7324-4a52-a478-488c865816cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64b5b47f-4e20-48c0-89ac-0b88ab4f11aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text file\n",
    "with open('RNN_text_dataset.txt', 'r', encoding='utf-8') as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f5c465a-bd04-48a4-99fc-eefc7e404dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Recurrent Neural Network RNN is a type of Neural Network where the output from the previous step is fed as input to the current step. In traditional neural networks, all the inputs and outputs are independent of each other. Still, in cases when it is required to predict the next word of a sentence, the previous words are required and hence there is a need to remember the previous words. Thus RNN came into existence, which solved this issue with the help of a Hidden Layer. The main and most important feature of RNN is its Hidden state, which remembers some information about a sequence. The state is also referred to as Memory State since it remembers the previous input to the network. It uses the same parameters for each input as it performs the same task on all the inputs or hidden layers to produce the output. This reduces the complexity of parameters, unlike other neural networks.\\n\\nArtificial neural networks that do not have looping nodes are called feed forward neural networks. Because all information is only passed forward, this kind of neural network is also referred to as a multi-layer neural network.\\n\\nInformation moves from the input layer to the output layer – if any hidden layers are present – unidirectionally in a feedforward neural network. These networks are appropriate for image classification tasks, for example, where input and output are independent. Nevertheless, their inability to retain previous inputs automatically renders them less useful for sequential data analysis.\\n\\nThe fundamental processing unit in a Recurrent Neural Network RNN is a Recurrent Unit, which is not explicitly called a “Recurrent Neuron.” This unit has the unique ability to maintain a hidden state, allowing the network to capture sequential dependencies by remembering previous inputs while processing. Long Short-Term Memory LSTM and Gated Recurrent Unit GRU) versions improve the RNN’s ability to handle long-term dependencies.\\n\\nThis type of RNN behaves the same as any simple Neural network it is also known as Vanilla Neural Network. In this Neural network, there is only one input and one output.\\n\\nIn this type of RNN, there is one input and many outputs associated with it. One of the most used examples of this network is Image captioning where given an image we predict a sentence having Multiple words. \\n\\nIn this type of network, Many inputs are fed to the network at several states of the network generating only one output. This type of network is used in the problems like sentimental analysis. Where we give multiple words as input and predict only the sentiment of the sentence as output. \\n\\nIn this type of neural network, there are multiple inputs and multiple outputs corresponding to a problem. One Example of this Problem will be language translation. In language translation, we provide multiple words from one language as input and predict multiple words from the second language as output.\\n\\nThe Recurrent Neural Network consists of multiple fixed activation function units, one for each time step. Each unit has an internal state which is called the hidden state of the unit. This hidden state signifies the past knowledge that the network currently holds at a given time step. This hidden state is updated at every time step to signify the change in the knowledge of the network about the past. The hidden state is updated using the following recurrence relation.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a8d2211-6cb5-4bad-8c98-6fe8e5cb0cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text_data])\n",
    "sequence_data = tokenizer.texts_to_sequences([text_data])\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb250e84-6a28-4f9b-9189-538f13651e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "max_sequence_len = max([len(x) for x in sequence_data])\n",
    "padded_sequence = pad_sequences(sequence_data, maxlen=max_sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a12d46c-7ee1-407f-bbe0-6b0da0d43f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sequence: [[ 26   5   2  19   4   7  20   3   5   2  30   1  15  31   1  21  27   4\n",
      "   50  10  12   6   1  85  27   9  86   5  28  39   1  22  11  40  16  51\n",
      "    3  32  52  87   9  88  89  23   4  53   6  33   1  90  91   3   7  41\n",
      "    1  21  24  16  53  11  92  34   4   7  93   6  94   1  21  24  95  19\n",
      "   96  97  98  35  99   8 100  54   1 101   3   7  13  36   1 102  11  55\n",
      "  103 104   3  19   4 105  13  14  35  56 106  42  57   7 107   1  14   4\n",
      "   43  58   6  10  59  14 108  23  56   1  21  12   6   1   2  23 109   1\n",
      "   44  60  29  32  12  10  23 110   1  44 111 112  39   1  22 113  13  61\n",
      "    6 114   1  15   8 115   1 116   3  60 117  52   5  28 118   5  28  62\n",
      "  119  63 120 121 122  16  45 123  64   5  28 124  39  42   4  37 125  64\n",
      "    8 126   3   5   2   4  43  58   6  10   7 127  36   5   2  42 128  31\n",
      "    1  12  36   6   1  15  36  65 129  66  13  61  16 130  65 131   9   7\n",
      "  132   5   2 133  28  16 134  29  46 135 136  29  67  30  12  11  15  16\n",
      "   51 137 138 139   6 140  21  22 141 142 143 144 145  29  68 146  69   1\n",
      "  147  70  25   9   7  26   5   2  19   4   7  26  25  35   4  63 148  45\n",
      "    7 149 150 151   8  25  71   1 152  72   6 153   7  13  14 154   1   2\n",
      "    6 155  68  73 156 157  21  22 158  70  74 159  75  59 160  11 161  26\n",
      "   25 162 163 164   1 165  72   6 166  74  75  73   8  20   3  19 167   1\n",
      "   44  10  66 168   5   2  23   4  43 169  10 170   5   2   9   8   5   2\n",
      "   34   4  37  17  12  11  17  15   9   8  20   3  19  34   4  17  12  11\n",
      "   76  40 171  54  23  17   3   1  55  77 172   3   8   2   4  46 173  30\n",
      "   78  79  46  47  33   7  41 174  18  24   9   8  20   3   2  76  22  16\n",
      "   50   6   1   2  48 175 176   3   1   2 177  37  17  15   8  20   3   2\n",
      "    4  77   9   1 178 179 180  69  30  47 181  18  24  10  12  11  33  37\n",
      "    1 182   3   1  41  10  15   9   8  20   3   5   2  34  16  18  22  11\n",
      "   18  40 183   6   7  80  17  67   3   8  80 184 185  38  81   9  38  81\n",
      "   47 186  18  24  31  17  38  10  12  11  33  18  24  31   1 187  38  10\n",
      "   15   1  26   5   2 188   3  18 189 190 191 192  17  29  32  49  27  32\n",
      "   25  71  79 193  14  35   4  45   1  13  14   3   1  25   8  13  14 194\n",
      "    1  82  83  62   1   2 195 196  48   7  78  49  27   8  13  14   4  84\n",
      "   48 197  49  27   6 198   1 199   9   1  83   3   1   2  57   1  82   1\n",
      "   13  14   4  84 200   1 201 202 203]]\n",
      "Word index: {'the': 1, 'network': 2, 'of': 3, 'is': 4, 'neural': 5, 'to': 6, 'a': 7, 'this': 8, 'in': 9, 'as': 10, 'and': 11, 'input': 12, 'hidden': 13, 'state': 14, 'output': 15, 'are': 16, 'one': 17, 'multiple': 18, 'rnn': 19, 'type': 20, 'previous': 21, 'inputs': 22, 'it': 23, 'words': 24, 'unit': 25, 'recurrent': 26, 'step': 27, 'networks': 28, 'for': 29, 'where': 30, 'from': 31, 'each': 32, 'predict': 33, 'there': 34, 'which': 35, 'layer': 36, 'only': 37, 'language': 38, 'all': 39, 'outputs': 40, 'sentence': 41, 'information': 42, 'also': 43, 'same': 44, 'called': 45, 'image': 46, 'we': 47, 'at': 48, 'time': 49, 'fed': 50, 'independent': 51, 'other': 52, 'required': 53, 'with': 54, 'most': 55, 'remembers': 56, 'about': 57, 'referred': 58, 'memory': 59, 'parameters': 60, 'layers': 61, 'that': 62, 'not': 63, 'forward': 64, '–': 65, 'any': 66, 'example': 67, 'sequential': 68, 'analysis': 69, 'processing': 70, 'has': 71, 'ability': 72, 'dependencies': 73, 'long': 74, 'term': 75, 'many': 76, 'used': 77, 'given': 78, 'an': 79, 'problem': 80, 'translation': 81, 'past': 82, 'knowledge': 83, 'updated': 84, 'current': 85, 'traditional': 86, 'still': 87, 'cases': 88, 'when': 89, 'next': 90, 'word': 91, 'hence': 92, 'need': 93, 'remember': 94, 'thus': 95, 'came': 96, 'into': 97, 'existence': 98, 'solved': 99, 'issue': 100, 'help': 101, 'main': 102, 'important': 103, 'feature': 104, 'its': 105, 'some': 106, 'sequence': 107, 'since': 108, 'uses': 109, 'performs': 110, 'task': 111, 'on': 112, 'or': 113, 'produce': 114, 'reduces': 115, 'complexity': 116, 'unlike': 117, 'artificial': 118, 'do': 119, 'have': 120, 'looping': 121, 'nodes': 122, 'feed': 123, 'because': 124, 'passed': 125, 'kind': 126, 'multi': 127, 'moves': 128, 'if': 129, 'present': 130, 'unidirectionally': 131, 'feedforward': 132, 'these': 133, 'appropriate': 134, 'classification': 135, 'tasks': 136, 'nevertheless': 137, 'their': 138, 'inability': 139, 'retain': 140, 'automatically': 141, 'renders': 142, 'them': 143, 'less': 144, 'useful': 145, 'data': 146, 'fundamental': 147, 'explicitly': 148, '“recurrent': 149, 'neuron': 150, '”': 151, 'unique': 152, 'maintain': 153, 'allowing': 154, 'capture': 155, 'by': 156, 'remembering': 157, 'while': 158, 'short': 159, 'lstm': 160, 'gated': 161, 'gru': 162, 'versions': 163, 'improve': 164, 'rnn’s': 165, 'handle': 166, 'behaves': 167, 'simple': 168, 'known': 169, 'vanilla': 170, 'associated': 171, 'examples': 172, 'captioning': 173, 'having': 174, 'several': 175, 'states': 176, 'generating': 177, 'problems': 178, 'like': 179, 'sentimental': 180, 'give': 181, 'sentiment': 182, 'corresponding': 183, 'will': 184, 'be': 185, 'provide': 186, 'second': 187, 'consists': 188, 'fixed': 189, 'activation': 190, 'function': 191, 'units': 192, 'internal': 193, 'signifies': 194, 'currently': 195, 'holds': 196, 'every': 197, 'signify': 198, 'change': 199, 'using': 200, 'following': 201, 'recurrence': 202, 'relation': 203}\n"
     ]
    }
   ],
   "source": [
    "# Output tokenized data for review\n",
    "print(\"Tokenized sequence:\", padded_sequence)\n",
    "print(\"Word index:\", word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4805e7b3-6143-499c-95b2-8b7b7f604c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Iqra\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the RNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(word_index) + 1, output_dim=128, input_length=max_sequence_len),\n",
    "    tf.keras.layers.SimpleRNN(128),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8653fc04-ebcf-4907-a26b-3cb89c3124de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0d6fca6-2b7a-41c7-a9ca-fc6e0aa8ebbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)               │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)               │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0842684-3249-47bc-bf28-0ca42d1bf39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models mentioned in the text: ['RNN', 'LSTM', 'GRU']\n"
     ]
    }
   ],
   "source": [
    "# Model name lookup\n",
    "model_names = [\"RNN\", \"LSTM\", \"GRU\"]\n",
    "found_models = [model for model in model_names if model.lower() in text_data.lower()]\n",
    "\n",
    "print(\"Models mentioned in the text:\", found_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb28e6b5-7b51-4a16-9353-949920967418",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([1])\n",
    "\n",
    "if len(labels) > 1:\n",
    "    # Split data into training and testing sets (use only if multiple samples are available)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(padded_sequence, labels, test_size=0.2, random_state=42)\n",
    "else:\n",
    "    # If only one sample, use the full dataset for training\n",
    "    X_train, X_test = padded_sequence, padded_sequence\n",
    "    y_train, y_test = labels, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fa21c60-0d16-41fe-a741-ae02bc22d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(word_index) + 1, output_dim=128, input_length=max_sequence_len),\n",
    "    tf.keras.layers.SimpleRNN(128),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80da8588-536c-4927-be81-8d1b76e80293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e320b6cd-57c4-4f0f-b62f-fd7a48da35d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 - 8s - 8s/step - accuracy: 1.0000 - loss: 0.6254 - val_accuracy: 1.0000 - val_loss: 0.3275\n",
      "Epoch 2/10\n",
      "1/1 - 0s - 310ms/step - accuracy: 1.0000 - loss: 0.3275 - val_accuracy: 1.0000 - val_loss: 0.2118\n",
      "Epoch 3/10\n",
      "1/1 - 0s - 317ms/step - accuracy: 1.0000 - loss: 0.2118 - val_accuracy: 1.0000 - val_loss: 0.1377\n",
      "Epoch 4/10\n",
      "1/1 - 0s - 315ms/step - accuracy: 1.0000 - loss: 0.1377 - val_accuracy: 1.0000 - val_loss: 0.1120\n",
      "Epoch 5/10\n",
      "1/1 - 0s - 309ms/step - accuracy: 1.0000 - loss: 0.1120 - val_accuracy: 1.0000 - val_loss: 0.0954\n",
      "Epoch 6/10\n",
      "1/1 - 0s - 333ms/step - accuracy: 1.0000 - loss: 0.0954 - val_accuracy: 1.0000 - val_loss: 0.1983\n",
      "Epoch 7/10\n",
      "1/1 - 0s - 334ms/step - accuracy: 1.0000 - loss: 0.1983 - val_accuracy: 1.0000 - val_loss: 0.3176\n",
      "Epoch 8/10\n",
      "1/1 - 0s - 325ms/step - accuracy: 1.0000 - loss: 0.3176 - val_accuracy: 1.0000 - val_loss: 0.2329\n",
      "Epoch 9/10\n",
      "1/1 - 0s - 325ms/step - accuracy: 1.0000 - loss: 0.2329 - val_accuracy: 1.0000 - val_loss: 0.0447\n",
      "Epoch 10/10\n",
      "1/1 - 0s - 329ms/step - accuracy: 1.0000 - loss: 0.0447 - val_accuracy: 1.0000 - val_loss: 0.0329\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6e7a55b-7c61-449e-8a7b-45dd7dd387a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 0.0329\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model (since it's the same data for training and testing in this case)\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dc6807-c988-4fd0-97b6-1f49bcb064b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
